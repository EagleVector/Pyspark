{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d667a9cd",
   "metadata": {},
   "source": [
    "# Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8ff688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ffdbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./env/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./env/lib/python3.8/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.8/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./env/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e51765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>120</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>Female</td>\n",
       "      <td>45</td>\n",
       "      <td>126</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "      <td>126</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "      <td>137</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>137</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CustomerID   Genre  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "0             1    Male   19                  15                      39\n",
       "1             2    Male   21                  15                      81\n",
       "2             3  Female   20                  16                       6\n",
       "3             4  Female   23                  16                      77\n",
       "4             5  Female   31                  17                      40\n",
       "..          ...     ...  ...                 ...                     ...\n",
       "195         196  Female   35                 120                      79\n",
       "196         197  Female   45                 126                      28\n",
       "197         198    Male   32                 126                      74\n",
       "198         199    Male   32                 137                      18\n",
       "199         200    Male   30                 137                      83\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('Mall_Customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df9a6b",
   "metadata": {},
   "source": [
    "# Creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31354297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0392828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/14 14:00:56 WARN Utils: Your hostname, shubhams-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.2.136 instead (on interface en0)\n",
      "22/11/14 14:00:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/14 14:00:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ff1cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.136:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11cb0f520>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d55132",
   "metadata": {},
   "source": [
    "spark is running in one data node local(one cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d76f11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Reading the dataframe in pyspark\n",
    "df_pyspark = spark.read.option('header','true').csv('Mall_Customers.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b148aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Annual Income (k$): integer (nullable = true)\n",
      " |-- Spending Score (1-100): integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the data type of the columns\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817a55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative way to read the data in pyspark\n",
    "df_pyspark=spark.read.csv('Mall_Customers.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cc6ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         2|  Male| 21|                15|                    81|\n",
      "|         3|Female| 20|                16|                     6|\n",
      "|         4|Female| 23|                16|                    77|\n",
      "|         5|Female| 31|                17|                    40|\n",
      "|         6|Female| 22|                17|                    76|\n",
      "|         7|Female| 35|                18|                     6|\n",
      "|         8|Female| 23|                18|                    94|\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        10|Female| 30|                19|                    72|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        12|Female| 35|                19|                    99|\n",
      "|        13|Female| 58|                20|                    15|\n",
      "|        14|Female| 24|                20|                    77|\n",
      "|        15|  Male| 37|                20|                    13|\n",
      "|        16|  Male| 22|                20|                    79|\n",
      "|        17|Female| 35|                21|                    35|\n",
      "|        18|  Male| 20|                21|                    66|\n",
      "|        19|  Male| 52|                23|                    29|\n",
      "|        20|Female| 35|                23|                    98|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffac2288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Annual Income (k$): integer (nullable = true)\n",
      " |-- Spending Score (1-100): integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.info()\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c085d182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "881cf7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.columns()\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9344aa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CustomerID=1, Genre='Male', Age=19, Annual Income (k$)=15, Spending Score (1-100)=39),\n",
       " Row(CustomerID=2, Genre='Male', Age=21, Annual Income (k$)=15, Spending Score (1-100)=81),\n",
       " Row(CustomerID=3, Genre='Female', Age=20, Annual Income (k$)=16, Spending Score (1-100)=6),\n",
       " Row(CustomerID=4, Genre='Female', Age=23, Annual Income (k$)=16, Spending Score (1-100)=77),\n",
       " Row(CustomerID=5, Genre='Female', Age=31, Annual Income (k$)=17, Spending Score (1-100)=40),\n",
       " Row(CustomerID=6, Genre='Female', Age=22, Annual Income (k$)=17, Spending Score (1-100)=76),\n",
       " Row(CustomerID=7, Genre='Female', Age=35, Annual Income (k$)=18, Spending Score (1-100)=6),\n",
       " Row(CustomerID=8, Genre='Female', Age=23, Annual Income (k$)=18, Spending Score (1-100)=94),\n",
       " Row(CustomerID=9, Genre='Male', Age=64, Annual Income (k$)=19, Spending Score (1-100)=3),\n",
       " Row(CustomerID=10, Genre='Female', Age=30, Annual Income (k$)=19, Spending Score (1-100)=72)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.head()\n",
    "df_pyspark.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe81ef9",
   "metadata": {},
   "source": [
    "Here we get the head in the form of list unlike pandas where we get it in the form of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51df238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|CustomerID|\n",
      "+----------+\n",
      "|         1|\n",
      "|         2|\n",
      "|         3|\n",
      "|         4|\n",
      "|         5|\n",
      "|         6|\n",
      "|         7|\n",
      "|         8|\n",
      "|         9|\n",
      "|        10|\n",
      "|        11|\n",
      "|        12|\n",
      "|        13|\n",
      "|        14|\n",
      "|        15|\n",
      "|        16|\n",
      "|        17|\n",
      "|        18|\n",
      "|        19|\n",
      "|        20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selecting one particular row in the dataframe.\n",
    "df_pyspark.select('CustomerID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b70b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+\n",
      "|CustomerID| Genre|Age|\n",
      "+----------+------+---+\n",
      "|         1|  Male| 19|\n",
      "|         2|  Male| 21|\n",
      "|         3|Female| 20|\n",
      "|         4|Female| 23|\n",
      "|         5|Female| 31|\n",
      "|         6|Female| 22|\n",
      "|         7|Female| 35|\n",
      "|         8|Female| 23|\n",
      "|         9|  Male| 64|\n",
      "|        10|Female| 30|\n",
      "|        11|  Male| 67|\n",
      "|        12|Female| 35|\n",
      "|        13|Female| 58|\n",
      "|        14|Female| 24|\n",
      "|        15|  Male| 37|\n",
      "|        16|  Male| 22|\n",
      "|        17|Female| 35|\n",
      "|        18|  Male| 20|\n",
      "|        19|  Male| 52|\n",
      "|        20|Female| 35|\n",
      "+----------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Multiple columns\n",
    "df_pyspark.select(['CustomerID', 'Genre', 'Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea546c78",
   "metadata": {},
   "source": [
    "Slicing is not possible in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c0e1aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Genre'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "735e17bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CustomerID', 'int'),\n",
       " ('Genre', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('Annual Income (k$)', 'int'),\n",
       " ('Spending Score (1-100)', 'int')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.dtypes\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a20a48bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------+-----------------+------------------+----------------------+\n",
      "|summary|        CustomerID| Genre|              Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+-------+------------------+------+-----------------+------------------+----------------------+\n",
      "|  count|               200|   200|              200|               200|                   200|\n",
      "|   mean|             100.5|  null|            38.85|             60.56|                  50.2|\n",
      "| stddev|57.879184513951124|  null|13.96900733155888| 26.26472116527124|    25.823521668370173|\n",
      "|    min|                 1|Female|               18|                15|                     1|\n",
      "|    max|               200|  Male|               70|               137|                    99|\n",
      "+-------+------------------+------+-----------------+------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.describe()\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79442bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Columns in dataframe\n",
    "df_pyspark = df_pyspark.withColumn('Age after 2 years', df_pyspark['Age']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68c70831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+-----------------+\n",
      "|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|Age after 2 years|\n",
      "+----------+------+---+------------------+----------------------+-----------------+\n",
      "|         1|  Male| 19|                15|                    39|               21|\n",
      "|         2|  Male| 21|                15|                    81|               23|\n",
      "|         3|Female| 20|                16|                     6|               22|\n",
      "|         4|Female| 23|                16|                    77|               25|\n",
      "|         5|Female| 31|                17|                    40|               33|\n",
      "|         6|Female| 22|                17|                    76|               24|\n",
      "|         7|Female| 35|                18|                     6|               37|\n",
      "|         8|Female| 23|                18|                    94|               25|\n",
      "|         9|  Male| 64|                19|                     3|               66|\n",
      "|        10|Female| 30|                19|                    72|               32|\n",
      "|        11|  Male| 67|                19|                    14|               69|\n",
      "|        12|Female| 35|                19|                    99|               37|\n",
      "|        13|Female| 58|                20|                    15|               60|\n",
      "|        14|Female| 24|                20|                    77|               26|\n",
      "|        15|  Male| 37|                20|                    13|               39|\n",
      "|        16|  Male| 22|                20|                    79|               24|\n",
      "|        17|Female| 35|                21|                    35|               37|\n",
      "|        18|  Male| 20|                21|                    66|               22|\n",
      "|        19|  Male| 52|                23|                    29|               54|\n",
      "|        20|Female| 35|                23|                    98|               37|\n",
      "+----------+------+---+------------------+----------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "653a0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the columns\n",
    "df_pyspark = df_pyspark.drop('Age after 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b309550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         2|  Male| 21|                15|                    81|\n",
      "|         3|Female| 20|                16|                     6|\n",
      "|         4|Female| 23|                16|                    77|\n",
      "|         5|Female| 31|                17|                    40|\n",
      "|         6|Female| 22|                17|                    76|\n",
      "|         7|Female| 35|                18|                     6|\n",
      "|         8|Female| 23|                18|                    94|\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        10|Female| 30|                19|                    72|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        12|Female| 35|                19|                    99|\n",
      "|        13|Female| 58|                20|                    15|\n",
      "|        14|Female| 24|                20|                    77|\n",
      "|        15|  Male| 37|                20|                    13|\n",
      "|        16|  Male| 22|                20|                    79|\n",
      "|        17|Female| 35|                21|                    35|\n",
      "|        18|  Male| 20|                21|                    66|\n",
      "|        19|  Male| 52|                23|                    29|\n",
      "|        20|Female| 35|                23|                    98|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78a9898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the columns\n",
    "df_pyspark = df_pyspark.withColumnRenamed('Genre', 'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a008845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         2|  Male| 21|                15|                    81|\n",
      "|         3|Female| 20|                16|                     6|\n",
      "|         4|Female| 23|                16|                    77|\n",
      "|         5|Female| 31|                17|                    40|\n",
      "|         6|Female| 22|                17|                    76|\n",
      "|         7|Female| 35|                18|                     6|\n",
      "|         8|Female| 23|                18|                    94|\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        10|Female| 30|                19|                    72|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        12|Female| 35|                19|                    99|\n",
      "|        13|Female| 58|                20|                    15|\n",
      "|        14|Female| 24|                20|                    77|\n",
      "|        15|  Male| 37|                20|                    13|\n",
      "|        16|  Male| 22|                20|                    79|\n",
      "|        17|Female| 35|                21|                    35|\n",
      "|        18|  Male| 20|                21|                    66|\n",
      "|        19|  Male| 52|                23|                    29|\n",
      "|        20|Female| 35|                23|                    98|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "523188f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pyspark = spark.read.csv('brittleness-index.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09d009d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TK104: string (nullable = true)\n",
      " |-- TK105: string (nullable = true)\n",
      " |-- TK107: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d21f3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|TK104|TK105|TK107|\n",
      "+-----+-----+-----+\n",
      "|  254|  263|  338|\n",
      "|  440|   NA|  470|\n",
      "|  501|   NA|  558|\n",
      "|  368|  451|  426|\n",
      "|  697|  709|  733|\n",
      "|  476|  542|  539|\n",
      "|  188|  223|  240|\n",
      "|  525|  659|  628|\n",
      "|  451|  689|  517|\n",
      "|  517|  509|  564|\n",
      "|  370|  321|  435|\n",
      "|   NA|  403|  306|\n",
      "|   NA|  690|  558|\n",
      "|   NA|  460|  358|\n",
      "|  396|  492|  429|\n",
      "|  279|  369|  291|\n",
      "|  450|  507|  520|\n",
      "|  422|  370|  491|\n",
      "|  406|  378|  489|\n",
      "|  425|  549|  479|\n",
      "+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fce21ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|TK105|TK107|\n",
      "+-----+-----+\n",
      "|  263|  338|\n",
      "|   NA|  470|\n",
      "|   NA|  558|\n",
      "|  451|  426|\n",
      "|  709|  733|\n",
      "|  542|  539|\n",
      "|  223|  240|\n",
      "|  659|  628|\n",
      "|  689|  517|\n",
      "|  509|  564|\n",
      "|  321|  435|\n",
      "|  403|  306|\n",
      "|  690|  558|\n",
      "|  460|  358|\n",
      "|  492|  429|\n",
      "|  369|  291|\n",
      "|  507|  520|\n",
      "|  370|  491|\n",
      "|  378|  489|\n",
      "|  549|  479|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop the columns\n",
    "data_pyspark.drop('TK104').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df7856e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the column values in columns\n",
    "data_pyspark = data_pyspark.na.replace('NA', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "029fde20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|TK104|TK105|TK107|\n",
      "+-----+-----+-----+\n",
      "|  254|  263|  338|\n",
      "|  440| null|  470|\n",
      "|  501| null|  558|\n",
      "|  368|  451|  426|\n",
      "|  697|  709|  733|\n",
      "|  476|  542|  539|\n",
      "|  188|  223|  240|\n",
      "|  525|  659|  628|\n",
      "|  451|  689|  517|\n",
      "|  517|  509|  564|\n",
      "|  370|  321|  435|\n",
      "| null|  403|  306|\n",
      "| null|  690|  558|\n",
      "| null|  460|  358|\n",
      "|  396|  492|  429|\n",
      "|  279|  369|  291|\n",
      "|  450|  507|  520|\n",
      "|  422|  370|  491|\n",
      "|  406|  378|  489|\n",
      "|  425|  549|  479|\n",
      "+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46a7b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TK104: float (nullable = true)\n",
      " |-- TK105: float (nullable = true)\n",
      " |-- TK107: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TypeCasting\n",
    "from pyspark.sql.types import FloatType\n",
    "  \n",
    "data_pyspark = data_pyspark \\\n",
    "  .withColumn(\"TK104\" ,\n",
    "              data_pyspark[\"TK104\"]\n",
    "              .cast(FloatType()))   \\\n",
    "  .withColumn(\"TK105\",\n",
    "              data_pyspark[\"TK105\"]\n",
    "              .cast(FloatType()))\n",
    "  \n",
    "data_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eda81c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|TK104|TK105|TK107|\n",
      "+-----+-----+-----+\n",
      "|254.0|263.0|  338|\n",
      "|368.0|451.0|  426|\n",
      "|697.0|709.0|  733|\n",
      "|476.0|542.0|  539|\n",
      "|188.0|223.0|  240|\n",
      "|525.0|659.0|  628|\n",
      "|451.0|689.0|  517|\n",
      "|517.0|509.0|  564|\n",
      "|370.0|321.0|  435|\n",
      "|396.0|492.0|  429|\n",
      "|279.0|369.0|  291|\n",
      "|450.0|507.0|  520|\n",
      "|422.0|370.0|  491|\n",
      "|406.0|378.0|  489|\n",
      "|425.0|549.0|  479|\n",
      "|544.0|631.0|  591|\n",
      "|376.0|291.0|  428|\n",
      "|335.0|410.0|  424|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop the null value columns\n",
    "data_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebdf9167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|TK104|TK105|TK107|\n",
      "+-----+-----+-----+\n",
      "|254.0|263.0|  338|\n",
      "|368.0|451.0|  426|\n",
      "|697.0|709.0|  733|\n",
      "|476.0|542.0|  539|\n",
      "|188.0|223.0|  240|\n",
      "|525.0|659.0|  628|\n",
      "|451.0|689.0|  517|\n",
      "|517.0|509.0|  564|\n",
      "|370.0|321.0|  435|\n",
      "|396.0|492.0|  429|\n",
      "|279.0|369.0|  291|\n",
      "|450.0|507.0|  520|\n",
      "|422.0|370.0|  491|\n",
      "|406.0|378.0|  489|\n",
      "|425.0|549.0|  479|\n",
      "|544.0|631.0|  591|\n",
      "|376.0|291.0|  428|\n",
      "|335.0|410.0|  424|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#any = how\n",
    "data_pyspark.na.drop(how = \"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ba46837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|TK104|TK105|TK107|\n",
      "+-----+-----+-----+\n",
      "|254.0|263.0|  338|\n",
      "|368.0|451.0|  426|\n",
      "|697.0|709.0|  733|\n",
      "|476.0|542.0|  539|\n",
      "|188.0|223.0|  240|\n",
      "|525.0|659.0|  628|\n",
      "|451.0|689.0|  517|\n",
      "|517.0|509.0|  564|\n",
      "|370.0|321.0|  435|\n",
      "|396.0|492.0|  429|\n",
      "|279.0|369.0|  291|\n",
      "|450.0|507.0|  520|\n",
      "|422.0|370.0|  491|\n",
      "|406.0|378.0|  489|\n",
      "|425.0|549.0|  479|\n",
      "|544.0|631.0|  591|\n",
      "|376.0|291.0|  428|\n",
      "|335.0|410.0|  424|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#thresh=x:at least x non-null values must be present\n",
    "data_pyspark.na.drop(how=\"any\", thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03c0ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|TK104|TK105|TK107|\n",
      "+-----+-----+-----+\n",
      "|254.0|263.0|  338|\n",
      "|440.0| null|  470|\n",
      "|501.0| null|  558|\n",
      "|368.0|451.0|  426|\n",
      "|697.0|709.0|  733|\n",
      "|476.0|542.0|  539|\n",
      "|188.0|223.0|  240|\n",
      "|525.0|659.0|  628|\n",
      "|451.0|689.0|  517|\n",
      "|517.0|509.0|  564|\n",
      "|370.0|321.0|  435|\n",
      "|396.0|492.0|  429|\n",
      "|279.0|369.0|  291|\n",
      "|450.0|507.0|  520|\n",
      "|422.0|370.0|  491|\n",
      "|406.0|378.0|  489|\n",
      "|425.0|549.0|  479|\n",
      "|544.0|631.0|  591|\n",
      "|376.0|291.0|  428|\n",
      "|335.0|410.0|  424|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subset:Dropping row from a particular column if missing\n",
    "data_pyspark.na.drop(how='any', subset=['TK104']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be2325ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|TK104|TK105|TK107|\n",
      "+-----+-----+-----+\n",
      "|254.0|263.0|  338|\n",
      "|440.0| null|  470|\n",
      "|501.0| null|  558|\n",
      "|368.0|451.0|  426|\n",
      "|697.0|709.0|  733|\n",
      "|476.0|542.0|  539|\n",
      "|188.0|223.0|  240|\n",
      "|525.0|659.0|  628|\n",
      "|451.0|689.0|  517|\n",
      "|517.0|509.0|  564|\n",
      "|370.0|321.0|  435|\n",
      "| null|403.0|  306|\n",
      "| null|690.0|  558|\n",
      "| null|460.0|  358|\n",
      "|396.0|492.0|  429|\n",
      "|279.0|369.0|  291|\n",
      "|450.0|507.0|  520|\n",
      "|422.0|370.0|  491|\n",
      "|406.0|378.0|  489|\n",
      "|425.0|549.0|  479|\n",
      "+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filling the missing values\n",
    "data_pyspark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d934668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|TK104|TK105|TK107|\n",
      "+-----+-----+-----+\n",
      "|254.0|263.0|  338|\n",
      "|440.0| null|  470|\n",
      "|501.0| null|  558|\n",
      "|368.0|451.0|  426|\n",
      "|697.0|709.0|  733|\n",
      "|476.0|542.0|  539|\n",
      "|188.0|223.0|  240|\n",
      "|525.0|659.0|  628|\n",
      "|451.0|689.0|  517|\n",
      "|517.0|509.0|  564|\n",
      "|370.0|321.0|  435|\n",
      "| null|403.0|  306|\n",
      "| null|690.0|  558|\n",
      "| null|460.0|  358|\n",
      "|396.0|492.0|  429|\n",
      "|279.0|369.0|  291|\n",
      "|450.0|507.0|  520|\n",
      "|422.0|370.0|  491|\n",
      "|406.0|378.0|  489|\n",
      "|425.0|549.0|  479|\n",
      "+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subset\n",
    "data_pyspark.na.fill('Missing Values', ['TK104']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbb88914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|TK104|TK105|TK107|\n",
      "+-----+-----+-----+\n",
      "|254.0|263.0|  338|\n",
      "|440.0| null|  470|\n",
      "|501.0| null|  558|\n",
      "|368.0|451.0|  426|\n",
      "|697.0|709.0|  733|\n",
      "|476.0|542.0|  539|\n",
      "|188.0|223.0|  240|\n",
      "|525.0|659.0|  628|\n",
      "|451.0|689.0|  517|\n",
      "|517.0|509.0|  564|\n",
      "|370.0|321.0|  435|\n",
      "| null|403.0|  306|\n",
      "| null|690.0|  558|\n",
      "| null|460.0|  358|\n",
      "|396.0|492.0|  429|\n",
      "|279.0|369.0|  291|\n",
      "|450.0|507.0|  520|\n",
      "|422.0|370.0|  491|\n",
      "|406.0|378.0|  489|\n",
      "|425.0|549.0|  479|\n",
      "+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35c51bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation in pyspark\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['TK104', 'TK105'],\n",
    "    outputCols=[\"{}_imputed\".format(col) for col in ['TK104', 'TK105']]\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf15ec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-------------+-------------+\n",
      "|TK104|TK105|TK107|TK104_imputed|TK105_imputed|\n",
      "+-----+-----+-----+-------------+-------------+\n",
      "|254.0|263.0|  338|        254.0|        263.0|\n",
      "|440.0| null|  470|        440.0|        460.0|\n",
      "|501.0| null|  558|        501.0|        460.0|\n",
      "|368.0|451.0|  426|        368.0|        451.0|\n",
      "|697.0|709.0|  733|        697.0|        709.0|\n",
      "|476.0|542.0|  539|        476.0|        542.0|\n",
      "|188.0|223.0|  240|        188.0|        223.0|\n",
      "|525.0|659.0|  628|        525.0|        659.0|\n",
      "|451.0|689.0|  517|        451.0|        689.0|\n",
      "|517.0|509.0|  564|        517.0|        509.0|\n",
      "|370.0|321.0|  435|        370.0|        321.0|\n",
      "| null|403.0|  306|        422.0|        403.0|\n",
      "| null|690.0|  558|        422.0|        690.0|\n",
      "| null|460.0|  358|        422.0|        460.0|\n",
      "|396.0|492.0|  429|        396.0|        492.0|\n",
      "|279.0|369.0|  291|        279.0|        369.0|\n",
      "|450.0|507.0|  520|        450.0|        507.0|\n",
      "|422.0|370.0|  491|        422.0|        370.0|\n",
      "|406.0|378.0|  489|        406.0|        378.0|\n",
      "|425.0|549.0|  479|        425.0|        549.0|\n",
      "+-----+-----+-----+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(data_pyspark).transform(data_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be261c42",
   "metadata": {},
   "source": [
    "# Filter Operation in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98ee11cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         2|  Male| 21|                15|                    81|\n",
      "|         3|Female| 20|                16|                     6|\n",
      "|         4|Female| 23|                16|                    77|\n",
      "|         5|Female| 31|                17|                    40|\n",
      "|         6|Female| 22|                17|                    76|\n",
      "|         7|Female| 35|                18|                     6|\n",
      "|         8|Female| 23|                18|                    94|\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        10|Female| 30|                19|                    72|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        12|Female| 35|                19|                    99|\n",
      "|        13|Female| 58|                20|                    15|\n",
      "|        14|Female| 24|                20|                    77|\n",
      "|        15|  Male| 37|                20|                    13|\n",
      "|        16|  Male| 22|                20|                    79|\n",
      "|        17|Female| 35|                21|                    35|\n",
      "|        18|  Male| 20|                21|                    66|\n",
      "|        19|  Male| 52|                23|                    29|\n",
      "|        20|Female| 35|                23|                    98|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e58c53ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        13|Female| 58|                20|                    15|\n",
      "|        19|  Male| 52|                23|                    29|\n",
      "|        23|Female| 46|                25|                     5|\n",
      "|        25|Female| 54|                28|                    14|\n",
      "|        27|Female| 45|                28|                    32|\n",
      "|        31|  Male| 60|                30|                     4|\n",
      "|        33|  Male| 53|                33|                     4|\n",
      "|        35|Female| 49|                33|                    14|\n",
      "|        37|Female| 42|                34|                    17|\n",
      "|        41|Female| 65|                38|                    35|\n",
      "|        43|  Male| 48|                39|                    36|\n",
      "|        45|Female| 49|                39|                    28|\n",
      "|        47|Female| 50|                40|                    55|\n",
      "|        51|Female| 49|                42|                    52|\n",
      "|        54|  Male| 59|                43|                    60|\n",
      "|        55|Female| 50|                43|                    45|\n",
      "|        56|  Male| 47|                43|                    41|\n",
      "|        57|Female| 51|                44|                    50|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Age of customerID more than 20\n",
    "df_pyspark.filter(\"Age > 40\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41d11d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|CustomerID|Gender|\n",
      "+----------+------+\n",
      "|         9|  Male|\n",
      "|        11|  Male|\n",
      "|        13|Female|\n",
      "|        19|  Male|\n",
      "|        23|Female|\n",
      "|        25|Female|\n",
      "|        27|Female|\n",
      "|        31|  Male|\n",
      "|        33|  Male|\n",
      "|        35|Female|\n",
      "|        37|Female|\n",
      "|        41|Female|\n",
      "|        43|  Male|\n",
      "|        45|Female|\n",
      "|        47|Female|\n",
      "|        51|Female|\n",
      "|        54|  Male|\n",
      "|        55|Female|\n",
      "|        56|  Male|\n",
      "|        57|Female|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"Age > 40\").select(['CustomerID', 'Gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8de93076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        41|Female| 65|                38|                    35|\n",
      "|        58|  Male| 69|                44|                    46|\n",
      "|        61|  Male| 70|                46|                    56|\n",
      "|        63|Female| 67|                47|                    52|\n",
      "|        65|  Male| 63|                48|                    51|\n",
      "|        68|Female| 68|                48|                    48|\n",
      "|        71|  Male| 70|                49|                    55|\n",
      "|        83|  Male| 67|                54|                    41|\n",
      "|        91|Female| 68|                59|                    55|\n",
      "|       103|  Male| 67|                62|                    59|\n",
      "|       107|Female| 66|                63|                    50|\n",
      "|       109|  Male| 68|                63|                    43|\n",
      "|       110|  Male| 66|                63|                    48|\n",
      "|       111|  Male| 65|                63|                    52|\n",
      "|       117|Female| 63|                65|                    43|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Age']>60).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd53f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        58|  Male| 69|                44|                    46|\n",
      "|        61|  Male| 70|                46|                    56|\n",
      "|        65|  Male| 63|                48|                    51|\n",
      "|        71|  Male| 70|                49|                    55|\n",
      "|        83|  Male| 67|                54|                    41|\n",
      "|       103|  Male| 67|                62|                    59|\n",
      "|       109|  Male| 68|                63|                    43|\n",
      "|       110|  Male| 66|                63|                    48|\n",
      "|       111|  Male| 65|                63|                    52|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Two different condition\n",
    "df_pyspark.filter((df_pyspark['Age']>60) & (df_pyspark['Gender'] == 'Male')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa2df323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        31|  Male| 60|                30|                     4|\n",
      "|        41|Female| 65|                38|                    35|\n",
      "|        58|  Male| 69|                44|                    46|\n",
      "|        61|  Male| 70|                46|                    56|\n",
      "|        63|Female| 67|                47|                    52|\n",
      "|        65|  Male| 63|                48|                    51|\n",
      "|        68|Female| 68|                48|                    48|\n",
      "|        71|  Male| 70|                49|                    55|\n",
      "|        73|Female| 60|                50|                    49|\n",
      "|        74|Female| 60|                50|                    56|\n",
      "|        83|  Male| 67|                54|                    41|\n",
      "|        91|Female| 68|                59|                    55|\n",
      "|       103|  Male| 67|                62|                    59|\n",
      "|       107|Female| 66|                63|                    50|\n",
      "|       109|  Male| 68|                63|                    43|\n",
      "|       110|  Male| 66|                63|                    48|\n",
      "|       111|  Male| 65|                63|                    52|\n",
      "|       117|Female| 63|                65|                    43|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Inverse filter operation in pyspark\n",
    "df_pyspark.filter(~(df_pyspark['Age']<60)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e00ef",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate operation in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76cd5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pyspark = spark.read.option('header','true').csv('test.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0db9a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c444231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85598a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GroupBy and aggregate function works together\n",
    "#Grouped to retrieve the max salary\n",
    "test_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1d6f979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GroupBy Departments which give max salary\n",
    "test_pyspark.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ed5fab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pyspark.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03585c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pyspark.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "566844b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Directly applying the aggregate function\n",
    "test_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9660b5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|min(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|       5000|\n",
      "|    Sunny|       2000|\n",
      "|    Krish|       4000|\n",
      "|   Mahesh|       3000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pyspark.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d5014e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pyspark.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b72a31",
   "metadata": {},
   "source": [
    "# Spark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb3bf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pyspark = spark.read.option('header','true').csv('ML.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0fa5db71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2278a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95a31cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pyspark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b451d1",
   "metadata": {},
   "source": [
    "In pyspark we dont do train-test split instead we use the VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb864408",
   "metadata": {},
   "source": [
    "['age','Experience']-------> new feature--------> independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa0d1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"age\", \"Experience\"], outputCol=\"Independent Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "433007f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler.transform(train_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4cd53123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+-------------------+\n",
      "|     Name|age|Experience|Salary|Independent Feature|\n",
      "+---------+---+----------+------+-------------------+\n",
      "|    Krish| 31|        10| 30000|        [31.0,10.0]|\n",
      "|Sudhanshu| 30|         8| 25000|         [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|         [29.0,4.0]|\n",
      "|     Paul| 24|         3| 20000|         [24.0,3.0]|\n",
      "|   Harsha| 21|         1| 15000|         [21.0,1.0]|\n",
      "|  Shubham| 23|         2| 18000|         [23.0,2.0]|\n",
      "+---------+---+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a17d385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary', 'Independent Feature']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35ce8935",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"Independent Feature\", \"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "880463ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|Independent Feature|Salary|\n",
      "+-------------------+------+\n",
      "|        [31.0,10.0]| 30000|\n",
      "|         [30.0,8.0]| 25000|\n",
      "|         [29.0,4.0]| 20000|\n",
      "|         [24.0,3.0]| 20000|\n",
      "|         [21.0,1.0]| 15000|\n",
      "|         [23.0,2.0]| 18000|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5e040ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/14 14:01:20 WARN Instrumentation: [8bba3332] regParam is zero, which might cause numerical instability and overfitting.\n",
      "22/11/14 14:01:21 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/11/14 14:01:21 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/11/14 14:01:21 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "#train-test split\n",
    "train_data, test_data = finalized_data.randomSplit([0.70,0.30])\n",
    "#Training\n",
    "regressor = LinearRegression(featuresCol='Independent Feature', labelCol='Salary')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ebf51f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-64.8464, 1584.7554])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d9e8186b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15414.10693970376"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a4dc554",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9de3e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------------------+\n",
      "|Independent Feature|Salary|        prediction|\n",
      "+-------------------+------+------------------+\n",
      "|         [24.0,3.0]| 20000|18612.059158134223|\n",
      "+-------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "00c7be50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1387.9408418657767, 1926379.780519081)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performance params\n",
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
